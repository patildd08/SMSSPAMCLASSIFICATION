# -*- coding: utf-8 -*-
"""DarshanNLPProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZB413D3Am8TxKY2mXUQYZx1PqkUQYwdh
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# %matplotlib inline
sns.set_style('darkgrid')
import nltk
nltk.download('punkt') # Download necessary resources
import nltk
nltk.download('averaged_perceptron_tagger')
import nltk
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

"""Loading the Data"""

sms = pd.read_csv('/content/SMSSpamCollection (1).csv', sep='\t',
                  names=["label", "message"])
sms.head()

# Open the file for reading
with open('/content/SMSSpamCollection (1).csv', 'r') as file:
    # Read the contents of the file into a string variable
    text = file.read()

# Tokenize the text into words
tokens = nltk.word_tokenize(text)# Tokenize the text into words

# Perform POS tagging on the tokens
pos_tags = nltk.pos_tag(tokens)

print(pos_tags)

lemmas = []
for token, tag in pos_tags:
    if tag.startswith('NN'):
        lemma = lemmatizer.lemmatize(token, pos='n')
    elif tag.startswith('VB'):
        lemma = lemmatizer.lemmatize(token, pos='v')
    elif tag.startswith('JJ'):
        lemma = lemmatizer.lemmatize(token, pos='a')
    else:
        lemma = token
    lemmas.append(lemma)

lemmatized_text = ' '.join(lemmas)
print(lemmatized_text)

"""Performing Exploratory Data Analysis"""

sms.describe()

"""Intotal there are 5572 messages."""

sms.groupby('label').describe()

"""Target variable is either ham or spam and there exists 4825 ham messages and 747 spam messages."""

plt.figure(figsize=(8,4))
sns.countplot(x='label', data=sms)

plt.title('Count Plot')

"""Text(0.5, 1.0, 'Count Plot')

Now we will focus on length of the messages
"""

sms['length'] = sms['message'].apply(len)
sms.head()

plt.figure(figsize=(8,4))
sns.distplot(sms[('length')])

"""Data has some outliers with more than 800 characters. box plot is used to discover the outliers."""

plt.figure(figsize=(8,2))
sns.boxplot(sms[('length')])

"""There are 3 messages with about 600 characters, 1 with 800 characters and 1 with 900 characters. """

sms[sms['length'] > 500]

for text in sms[sms['length'] > 550]['message']:
    print(text, "\n\n")

g = sns.FacetGrid(data=sms, hue="label", height=4, aspect=2)
g.map(sns.distplot, 'length', bins=30)
g.set(xticks=np.arange(0,1000,50))
plt.legend()

"""Average length of harm messages is about 40 characters while that of spam messages is 160. there is big difference, so length is a good feature to classify message labels.

- Creating Model
Text Pre-processing

Before vectorizing the messages, we will clean them to get the words we actually want by removing punctuation and stop words (i.e. "the", "a", "to"...). This process is called tokenization we will need the NLTK library to do this step.
"""

import string
import nltk
from nltk.corpus import stopwords
def text_preprocess(text):
    remove_punctuation = "".join([c for c in text if c not in string.punctuation])
    remove_stopwords = [word for word in remove_punctuation.split() if word not in stopwords.words('english')]
    
    return remove_stopwords

import nltk
nltk.download('stopwords')

sms['message'].head(10)

sms['message'].head(10).apply(text_preprocess)

"""```
`# This is formatted as code`
```

Create **Model**

In this step we will create a pipeline, in which:

first, we use CountVectorizer to convert text messages into a matrix of token counts, where one dimension is all the words in the data, and the other is all the messages,
then, we calculate term frequency-inverse document frequency (TF-IDF), which measures the importance of each word to each message in the whole data,
finally, we use Naive Bayes classifier model to train and predict the data.
But first we need to split the data into train and test data.
"""

#Train the test split
from sklearn.model_selection import train_test_split

X = sms['message']
y= sms['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42)

#Create the pipeline
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB

pipeline = Pipeline([
    ('vectorize', CountVectorizer(analyzer=text_preprocess)),
    ('tfidf', TfidfTransformer()),
    ('NBclassifier', MultinomialNB())
])

#Train the model
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

#Evaluate the model
from sklearn.metrics import confusion_matrix, classification_report

print(f"""
Confusion Matrix:
{confusion_matrix(y_test, y_pred)}

Classification Report:
{classification_report(y_test, y_pred)}
""")

"""There are intotal 66 messages that the model fails to predict as spam in the total of 246 spam messages in the dataset but the model does not misclassify any normal messages as spam. The overall accuracy rate is 96%."""