import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

%matplotlib inline
sns.set_style('darkgrid')

#Loading the Data
sms = pd.read_csv('/content/SMSSpamCollection (1).csv', sep='\t',
                  names=["label", "message"])
sms.head()

sms.describe()

#Performing Exploratory Data Analysis
sms.describe()
#Intotal there are 5572 messages.
sms.groupby('label').describe()
#Target variable is either ham or spam and there exists 4825 ham messages and 747 spam messages.
plt.figure(figsize=(8,4))
sns.countplot(x='label', data=sms)

plt.title('Count Plot')

#Now we will focus on length of the messages
sms['length'] = sms['message'].apply(len)
sms.head()
plt.figure(figsize=(8,4))
sns.distplot(sms[('length')])
#Data has some outliers with more than 800 characters. box plot is used to discover the outliers.
plt.figure(figsize=(8,2))
sns.boxplot(sms[('length')])
#There are 3 messages with about 600 characters, 1 with 800 characters and 1 with 900 characters. 
sms[sms['length'] > 500]
for text in sms[sms['length'] > 550]['message']:
    print(text, "\n\n")      
g = sns.FacetGrid(data=sms, hue="label", height=4, aspect=2)
g.map(sns.distplot, 'length', bins=30)
g.set(xticks=np.arange(0,1000,50))
plt.legend()

#Average length of ham messages is about 40 characters while that of spam messages is 160. there is big difference, so length is a good feature to classify message labels.
import string
import nltk
from nltk.corpus import stopwords
# nltk.download_shell() #download stopwords
def text_preprocess(text):
    remove_punctuation = "".join([c for c in text if c not in string.punctuation])
    remove_stopwords = [word for word in remove_punctuation.split() if word not in stopwords.words('english')]
    
    return remove_stopwords

import nltk
nltk.download('stopwords')
sms['message'].head(10)
sms['message'].head(10).apply(text_preprocess)
#Create Model
#Train the test split
from sklearn.model_selection import train_test_split

X = sms['message']
y= sms['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42)
 #Create the pipeline
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB

pipeline = Pipeline([
    ('vectorize', CountVectorizer(analyzer=text_preprocess)),
    ('tfidf', TfidfTransformer()),
    ('NBclassifier', MultinomialNB())
])
#Train the model
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

#Evaluate the model
from sklearn.metrics import confusion_matrix, classification_report

print(f"""
Confusion Matrix:
{confusion_matrix(y_test, y_pred)}

Classification Report:
{classification_report(y_test, y_pred)}
""")
